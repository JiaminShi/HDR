{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 6: High-Dynamic Range Imaging\n",
    "### Jiamin Shi, Benjamin M. Winger, Jingye Xu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "We Implemented Debevec-Malik method of recovering high dynamic range radiance maps from photographs taken with different amounts of exposure. Once radiance maps was computed, we could compress multiple photographs into one low dynamic range gamut by tone mapping. We first demonstrated the algorithm with known exposure time and aligned image, then we dropped the the assumption that exposure time was known. Finally, we extended the method to images taken without tripods using homography-based registration of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Modern cameras cannot capture as many details as the human eye can see in any given scene, especially under challenging light conditions. Photographic sensor or film can only capture a range of radiance value that is less than the nature world contains. To recover a full dynamic range image, people can take a set of bracketed exposures photographs. Because the mapping between rediance to pixal value is non-linear and unknonw, the challenge is that how we can combine these images together.\n",
    "\n",
    "We used Debevec-Malik method to estimate radiometric response function and irradiance value together. Since it is expensive and unnessarly to estimate all pixels, we chose random sampling, uniform sampling and variance-driven sampling and compared their result. Then, we recovered the radiance map by blending pixels from diffrent exposures with the weight function proposed by Mitsunaga and Nayar(1999), which maximized the signal-to-noise ratio. Finally, we use both global mapping and local mapping to display the radiance map on a lower dynamic range.\n",
    "\n",
    "Even if Debevec-Malik method assume that exposure time is known, we could also estimate exposure time along with radiometric response function by adding constraint terms, as long as we knew the relative relationship of exposure among images. To make our approach more general, we could also pre-process multi-exposure images taken without tripod by homography transform. Furthermore, it is possible to extend it to HDR panorama.\n",
    "\n",
    "The project is structured as following:\n",
    "\n",
    "1.0 The Algorithm\n",
    "\n",
    "- 1.1 Estimate the radiometric response function from the aligned images\n",
    "- 1.2 Estimate a radiance map by selecting of blending pixels from different exposures\n",
    "- 1.3 Tone map the resulting high dynamic range (HDR) image back into a displayable gamut\n",
    "\n",
    "2.0  Extension:\n",
    "\n",
    "- 2.1 Drop the assumption that exposure is known\n",
    "- 2.2 Images taken without tripod\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributions\n",
    "\n",
    "\n",
    "|         Author         |        Contribution       | \n",
    "| -------------------|---------------|\n",
    "| Jiamin Shi 20649720| 1.1(estimate response function), 1.3, 2.1, 2.2, abstract&introduction, pipeline integration | \n",
    "| Benjamin M. Winger | 1.2(improvement of weight function), version control configuration | \n",
    "| Jingye Xu 20705168 | 1.2, 1.1(implementation/analysis sample methods)| \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import skimage\n",
    "from skimage import color\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.transform import warp, ProjectiveTransform, SimilarityTransform\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "import matplotlib.image as image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "# source: https://www.easyhdr.com/examples\n",
    "im_auto_expose = image.imread(\"images/laurenziana_0.jpg\")\n",
    "im_under_expose = image.imread(\"images/laurenziana_-2.jpg\")\n",
    "im_over_expose = image.imread(\"images/laurenziana_+2.jpg\")\n",
    "\n",
    "imagebracket = np.array([im_under_expose,im_auto_expose,im_over_expose])\n",
    "\n",
    "log_t = [-2,0,2]\n",
    "\n",
    "# plot \n",
    "fig = plt.figure(figsize=(10,16))\n",
    "plt.subplot(221)\n",
    "plt.imshow(im_auto_expose)\n",
    "plt.title(\"auto setting exposure image(0)\")\n",
    "plt.subplot(222)\n",
    "plt.imshow(im_under_expose)\n",
    "plt.title(\"under-exposure image(-2)\")\n",
    "plt.subplot(223)\n",
    "plt.imshow(im_over_expose)\n",
    "plt.title(\"over-exposure image(+2)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1  Estimate the radiometric response function from the aligned images\n",
    "\n",
    "Estimate irradiance values $E_i$ and the radiometric response function $f$ at the same time. \n",
    "\n",
    "$$z_{ij} = f(E_i,t_j)$$ where $t_j$ is the exposure time for the $j$th image. \n",
    "\n",
    "The inverse response curve $f^{−1}$ is given by\n",
    "$$f^{−1}(z_{ij}) = E_i t_j$$\n",
    "\n",
    "Taking logarithms of both sides \n",
    "$$g(z_{ij})=\\log f^{-1}(z_{ij})=\\log E_i +\\log t_j$$\n",
    " ($g$ maps pixel values $z_{ij}$ into log irradiance)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we need make the reponse curve smooth by adding a second-order smoothness constraint\n",
    "$$\\lambda\\sum_k g''(k)^2 = \\lambda\\sum[g(k-1) - 2g(k)+ g(k+1)]^2$$\n",
    "\n",
    "Since pixel values are more reliable in the middile of their range, they also add a weight function\n",
    "$$ w(z)=\\left\\{\n",
    "\\begin{aligned}\n",
    "z-z_{min} & &  z \\le (z_{min}+z_{max})/2 \\\\\n",
    "z_{max}-z & & z \\gt (z_{min}+z_{max})/2 \\\\\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put all together we get a least squares problem to estimate the radiometric response function $g$ and irradiance values $E_i$\n",
    "\n",
    "$$E = \\sum_i\\sum_jw(z_{i,j})[g(z_{i,j}) - \\log E_i - \\log t_j]^2 + \\lambda\\sum_k g''(k)^2$$\n",
    "\n",
    "In other word, we are solve the two equations \n",
    "$$ w(z_{i,j}) g(z_{i,j}) - w(z_{i,j}) \\log E_i  = w(z_{i,j}) \\log t_j $$ \n",
    "$$\\lambda[g(z_{i,j}-1) - 2g(z_{i,j})+ g(z_{i,j}+1)] = 0$$\n",
    "together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assume that $t_j$ is known\n",
    "\n",
    "The response value $g_k = g(k)$, where g can be discretized according to the 256 pixel values commonly observed in eight-bit images. (The response curves are calibrated separately for each color channel.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Modified from gsolve.m\n",
    "\n",
    "Solve for imaging system response function\n",
    "\n",
    "Given a set of pixel values observed for several pixels in several\n",
    "images with different exposure times, this function returns the\n",
    "imaging system’s response function\n",
    "\n",
    "\n",
    "Z(i,j): the pixel values of pixel location number i in image j\n",
    "\n",
    "B(j): the log delta t, or log shutter speed, for image j\n",
    "\n",
    "l: lamdba, the constant that determines the amount of smoothness\n",
    "\n",
    "w(z): the weighting function value for pixel value z\n",
    "\n",
    "'''\n",
    "zmin = 0.\n",
    "zmax = 255.\n",
    "\n",
    "def weight_hat(z):\n",
    "    return min(z-zmin, zmax-z)\n",
    "\n",
    "def gsolve(Z, B, lmd, w=weight_hat):\n",
    "\n",
    "    locations = Z.shape[0]\n",
    "    sequences = Z.shape[1]\n",
    "    n = 256  # [0, 255]\n",
    "    A = np.zeros((locations * sequences + n - 1, locations + n), dtype=float)\n",
    "    b = np.zeros(A.shape[0], dtype=float)\n",
    "\n",
    "    #  Include the data−fitting equations\n",
    "    k = 0\n",
    "    for i in range(locations):\n",
    "        for j in range(sequences): \n",
    "            wij = w(Z[i, j])\n",
    "            A[k, int(Z[i, j])] = wij\n",
    "            A[k, n + i] = -wij\n",
    "            b[k] = wij * B[j]\n",
    "            k += 1\n",
    "\n",
    "    # Fix the curve by setting its middle value to 0, i.e. g(128) = 0    \n",
    "    A[k, 128] = 1   \n",
    "    k += 1\n",
    "\n",
    "    # Include the smoothness equations\n",
    "    for i in range(n-2): #(0, 253) 254 equations\n",
    "        wi = w(i + 1)\n",
    "        A[k, i] = lmd * wi\n",
    "        A[k, i+1] = -2 * lmd * wi\n",
    "        A[k, i+2] = lmd * wi\n",
    "        k += 1\n",
    "\n",
    "    # Solve the system\n",
    "    x = np.linalg.lstsq(A, b)[0]\n",
    "    g = x[:n]\n",
    "    lnE = x[n:]\n",
    "\n",
    "    return (g, lnE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagepack is to flat each input RGB image into a vector for each channle and pack them into three 2d array\n",
    "# input: np-array\n",
    "# return three Z(i,j)(see above) for each channel \n",
    "def imagepack(imagearray):\n",
    "    num_image = imagearray.shape[0]\n",
    "    # imagesize refer to 1-d image size\n",
    "    imagesize = imagearray[0].shape[0]*imagearray[0].shape[1]\n",
    "    imagepack = np.zeros((3, imagesize,num_image))\n",
    "    # naive loop ; can be optimized later\n",
    "    for i in range(num_image):\n",
    "        for j in range(3):\n",
    "            imagepack[j,:,i] = np.ndarray.flatten(imagearray[i][:,:,j])\n",
    "    return imagepack\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RP,GP,BP = imagepack(imagebracket) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample\n",
    "\n",
    "Question: how do we sample the pixel value $z_{ij}$? What is the sample ratio?\n",
    "\n",
    "The linear system should be overdetermined. For $N$ sample pixels in each image and $P$ images, we need $N \\times P > (Z_{max} -Z_{min}) + N$ (i.e. number of given parameters is greater than number of unknowns)\n",
    "\n",
    "Suppose we have 3 images, $2N > 255$, $N > 128 $ should be sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weakness/Strength Of Difference Sample Methods\n",
    "\n",
    "\n",
    "For randomSample method - pick pixel randomly\n",
    "\n",
    "- Weakness: not stable - some random result will lead to very bad estimate, for example, only sampling pixels with intensity 10\n",
    "\n",
    "- Strength: very quick and if our sample size is big enough, the result will be representative\n",
    "\n",
    "\n",
    "For windowSample method - pick pixel randomly:\n",
    "\n",
    "- Weakness: not so stable and having no idea how intensity distributes among the whole images. In some extreme case, it may fail to sample some pixel with certain intensity. Also, as Debevec and Malik 1997 indicated we better avoid highly-variance areas, this schema may visit those bad areas\n",
    "\n",
    "- Strength: very quick and it also goes through the whole images and in some way, it will be representative. \n",
    "\n",
    "For pixelCoverSample method - random select k pixels for each pixel value ([0,255])\n",
    "\n",
    "- Weakness: slow and have to go through the whole image to know the capacity of the k. Also it require shuffle/couting schema to do randomly sample. Fataly sometimes this schema does not work if any intensity is missing\n",
    "\n",
    "- Strength: all different intensity pixels get reported. Much more representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample function\n",
    "# general template:\n",
    "# inputimg: np.array - like Z(i,j)(see above) ; parameters for different sampling schemas\n",
    "# output: sampled input\n",
    "\n",
    "\n",
    "# random chose \"outputsize\" pixels\n",
    "def randomSample(inputimg,outputsize):\n",
    "    outputsize = int(outputsize)\n",
    "    indexrange = inputimg.shape[0]\n",
    "    num_image = inputimg.shape[1]\n",
    "    output = np.zeros((outputsize,num_image))\n",
    "    sampleindex = np.random.choice(indexrange,size=outputsize,replace=False)\n",
    "    for i in range(num_image):\n",
    "        output[:,i] = inputimg[sampleindex,i]\n",
    "    return output\n",
    "\n",
    "\n",
    "# choose pixels per windowsize where windowsize = floor(num of pixel / outputsize)\n",
    "def windowSample(inputimg,outputsize):\n",
    "    outputsize = int(outputsize)\n",
    "    indexrange = inputimg.shape[0]\n",
    "    num_image = inputimg.shape[1]\n",
    "    windowsize = int(indexrange/outputsize)\n",
    "    output = np.zeros((outputsize,num_image))\n",
    "    sampleindex = np.arange(outputsize) * windowsize\n",
    "    for i in range(num_image):\n",
    "        output[:,i] = inputimg[sampleindex,i]\n",
    "    return output\n",
    "\n",
    "######## warning: \n",
    "######## pixelCoverSample is not so good since sometimes, some certain intensity value will be absent\n",
    "######## for exmaple, int the toy image, we don't have 0 \n",
    "######## might try some more tolerant sampling schema\n",
    "\n",
    "# random select k pixels for each pixel value ([0,255])\n",
    "# base: using which image as a standard to pick index\n",
    "# if k is too larger, the output will base on the maxmum possible k value\n",
    "def pixelCoverSample(inputimg,k,base = 1):\n",
    "    indexrange = inputimg.shape[0]\n",
    "    num_image = inputimg.shape[1]\n",
    "    baseimg = inputimg[:,base]\n",
    "    print(baseimg)\n",
    "    pixelrange = 256\n",
    "    # i copy the inputimg since i will call shuffle later, shuffle is a in-place function\n",
    "    imgcopy = np.copy(inputimg)\n",
    "    \n",
    "    # check whether input k is feasible\n",
    "    freqcount = np.zeros(pixelrange)\n",
    "    for i in range(indexrange):\n",
    "        freqcount[int(baseimg[i])] = freqcount[int(baseimg[i])] + 1\n",
    "    maxk = np.amin(freqcount)\n",
    "    kval = k if maxk >= k else maxk\n",
    "    if kval != k:\n",
    "        print(\"warning: the given k is more than the capacity!\")\n",
    "    if kval == 0:\n",
    "        print(\"warning: the capacity is zero\")\n",
    "        return\n",
    "    # sample part:\n",
    "    # i used an inelegant method ; can improve later\n",
    "    # better idea - store some info while check whther k is feasible\n",
    "    # Or we can use some buind-in liabary - I did not find yet\n",
    "\n",
    "    output = np.zeros((kval * pixelrange,num_image))\n",
    "    # shuffle the imgs so that we have a random behavior\n",
    "    np.apply_along_axis(np.random.shuffle,1,imgcopy)\n",
    "    coutmap = np.full((pixelrange*kval),kval)\n",
    "    # i is the index of imgcopy\n",
    "    # j is the number of elements we already have in output\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while True:\n",
    "        if j == pixelrange*kval:\n",
    "            break\n",
    "        else:\n",
    "            tempval = imgcopy[i]\n",
    "            if coutmap[int(tempval[base])] != 0:\n",
    "                output[j,:] = imgcopy[i,:]\n",
    "                coutmap[int(tempval[base])] = coutmap[int(tempval[base])] - 1\n",
    "                j = j + 1\n",
    "            i = i + 1\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance-driven sampling\n",
    " \n",
    "Debevec and Malik(1997) indicated \"Furthermore, the pixels are best sampled from regions of the image with low intensity variance so that radiance can be assumed to be constant across the area of the pixel, and the effect of optical blur of the imaging system is minimized\".\n",
    "\n",
    "In this, we can introduce a biased sample schema that the probablity $P$ of each point $X_{ij}$ get sampled is based on its local gradient\n",
    "\n",
    "\n",
    "$$W_{i,j} = 1 - sigmoid(||\\nabla X_{ij}||)$$\n",
    "\n",
    "$$p(X_{ij}) = \\frac{W_{ij}}{\\sum_{p,q}W_{p,q}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use sigmoid-like function to map our gradient into probality\n",
    "# subjective to change\n",
    "# maybe slow due to floating number\n",
    "def decreaseSigmoid(a):\n",
    "    return 1 - 1/(1 + np.exp(-1*a))\n",
    "\n",
    "vfunc = np.vectorize(decreaseSigmoid)\n",
    "\n",
    "x = np.matrix( [[1,2],[2,3]])\n",
    "y = vfunc(x)\n",
    "y = np.sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input:\n",
    "# imgpack: data from imagePace\n",
    "# base: choose which image as basis to caculate local variance\n",
    "# shape: orignal shape\n",
    "# output:\n",
    "# 1-d array - the gradient of the image \n",
    "def GradientMap(imgpack,shape,base=1):\n",
    "    baseimg = imgpack[:,base]\n",
    "    baseimg = baseimg.reshape(shape)\n",
    "    yGadient = np.gradient(baseimg,axis =0)\n",
    "    xGadient = np.gradient(baseimg,axis =1)\n",
    "    result = np.sqrt(yGadient*yGadient + xGadient*xGadient)\n",
    "    return result\n",
    "\n",
    "# input: \n",
    "# Gradient:data from GradientMap\n",
    "# functionvector: function vector applying to each pixel in the image; see numpy.vectorize\n",
    "# output: \n",
    "# the weight map for each pixel\n",
    "def ProbMap(Gradient, functionvector):\n",
    "    result = functionvector(Gradient)\n",
    "    result = result / np.sum(result)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test block - visualize result \n",
    "GradientResult = GradientMap(RP,(im_auto_expose.shape[0],im_auto_expose.shape[1]))\n",
    "\n",
    "plt.figure(figsize = (6, 4))\n",
    "plt.title(\"gradient map - red channel of\")\n",
    "plt.imshow(GradientResult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block visualizes the result of probability map. Bright points have more chance to get sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize result\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.subplot(121)\n",
    "plt.title(\"auto setting exposure image(0)\")\n",
    "plt.imshow(im_auto_expose)\n",
    "plt.subplot(122)\n",
    "plt.title(\"probability map of red channel\")\n",
    "im = plt.imshow(promap)\n",
    "plt.colorbar(im,fraction=0.030)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### sampling block\n",
    "# input:\n",
    "# imgPack: data from imagePack\n",
    "# probmap: data from ProbMap\n",
    "# ouputsize: size of the sample \n",
    "# output:\n",
    "# sampled imagePack\n",
    "def gradientDrivenSample(imgPack,probmap,outputsize):\n",
    "    pvector = np.ndarray.flatten(probmap)\n",
    "    outputsize = int(outputsize)\n",
    "    indexrange = imgPack.shape[0]\n",
    "    num_image = imgPack.shape[1]\n",
    "    output = np.zeros((outputsize,num_image))\n",
    "    sampleindex = np.random.choice(indexrange,size=outputsize,replace=False,p=pvector)\n",
    "    for i in range(num_image):\n",
    "        output[:,i] = imgPack[sampleindex,i]\n",
    "    return output  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the log response function and result image\n",
    "\n",
    "If the images are noise free, we can use any non-saturated pixel value to estimate the corresponding radiance by mapping it through the inverse response curve $E = g(z)$.\n",
    "\n",
    "Debevec and Malik (1997) use a hat function (10.7) which accentuates mid-tone pixels while avoiding saturated values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib notebook\n",
    "\n",
    "lmd = 100\n",
    "#### we use window sample here\n",
    "targetsize = 1000 #(im_auto_expose.shape[1] * im_auto_expose.shape[0]) / 100\n",
    "sampledRP = windowSample(RP,targetsize)\n",
    "sampledGP = windowSample(GP,targetsize)\n",
    "sampledBP = windowSample(BP,targetsize)\n",
    "Rg, RinE = gsolve(sampledRP, log_t, lmd)\n",
    "Gg, GinE = gsolve(sampledGP, log_t, lmd)\n",
    "Bg, BinE = gsolve(sampledBP, log_t, lmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib notebook\n",
    "#### use gradient drive sample schema here\n",
    "\n",
    "RPpromap = ProbMap(GradientMap(RP,(im_auto_expose.shape[0],im_auto_expose.shape[1])),vfunc)\n",
    "GPpromap = ProbMap(GradientMap(GP,(im_auto_expose.shape[0],im_auto_expose.shape[1])),vfunc)\n",
    "BPpromap = ProbMap(GradientMap(BP,(im_auto_expose.shape[0],im_auto_expose.shape[1])),vfunc)\n",
    "\n",
    "vDsampledRP = gradientDrivenSample(RP,RPpromap,targetsize)\n",
    "vDsampledGP = gradientDrivenSample(GP,GPpromap,targetsize)\n",
    "vDsampledBP = gradientDrivenSample(BP,BPpromap,targetsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructed curves by window sample schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib notebook\n",
    "#### we use radient drive sample here\n",
    "#### notice this can be very slow\n",
    "vDRg, vDRinE = gsolve(vDsampledRP,[-2,0,2], lmd)\n",
    "vDGg, vDGinE = gsolve(vDsampledGP,[-2,0,2], lmd)\n",
    "vDBg, vDBinE = gsolve(vDsampledBP,[-2,0,2], lmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelrange = np.arange(256)\n",
    "\n",
    "fig = plt.figure(figsize=(6,12))\n",
    "plt.subplot(311)\n",
    "plt.plot(RinE, sampledRP[:,0], 'x', color = 'khaki', label='image1')\n",
    "plt.plot(RinE, sampledRP[:,1], 'x', color = 'palegreen', label='image2')\n",
    "plt.plot(RinE, sampledRP[:,2], 'x', color = 'skyblue', label='image3')\n",
    "plt.plot(Rg, pixelrange, color='red')\n",
    "plt.legend()\n",
    "plt.title(\"Pixel value over log exposure (Red) - window sample schema\")\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.plot(GinE, sampledGP[:,0], 'x', color = 'khaki', label='image1')\n",
    "plt.plot(GinE, sampledGP[:,1], 'x', color = 'palegreen', label='image2')\n",
    "plt.plot(GinE, sampledGP[:,2], 'x', color = 'skyblue', label='image3')\n",
    "plt.plot(Gg, pixelrange, color='green')\n",
    "plt.legend()\n",
    "plt.title(\"Pixel value over log exposure (Green) - window sample schema\")\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.plot(BinE, sampledBP[:,0], 'x', color = 'khaki', label='image1')\n",
    "plt.plot(BinE, sampledBP[:,1], 'x', color = 'palegreen', label='image2')\n",
    "plt.plot(BinE, sampledBP[:,2], 'x', color = 'skyblue', label='image3')\n",
    "plt.plot(Bg, pixelrange, color='blue')\n",
    "plt.legend()\n",
    "plt.title(\"Pixel value over log exposure (Blue) - window sample schema\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructed curves by drive sample schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,12))\n",
    "plt.subplot(311)\n",
    "plt.plot(vDRinE, vDsampledRP[:,0], 'x', color = 'khaki', label='image1')\n",
    "plt.plot(vDRinE, vDsampledRP[:,1], 'x', color = 'palegreen', label='image2')\n",
    "plt.plot(vDRinE, vDsampledRP[:,2], 'x', color = 'skyblue', label='image3')\n",
    "plt.plot(vDRg, pixelrange, color='red')\n",
    "plt.legend()\n",
    "plt.title(\"Pixel value over log exposure (Red) - gradient-drive sample schema\")\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.plot(vDGinE, vDsampledGP[:,0], 'x', color = 'khaki', label='image1')\n",
    "plt.plot(vDGinE, vDsampledGP[:,1], 'x', color = 'palegreen', label='image2')\n",
    "plt.plot(vDGinE, vDsampledGP[:,2], 'x', color = 'skyblue', label='image3')\n",
    "plt.plot(vDGg, pixelrange, color='green')\n",
    "plt.legend()\n",
    "plt.title(\"Pixel value over log exposure (Green) - gradient-drive sample schema\")\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.plot(vDBinE, vDsampledBP[:,0], 'x', color = 'khaki', label='image1')\n",
    "plt.plot(vDBinE, vDsampledBP[:,1], 'x', color = 'palegreen', label='image2')\n",
    "plt.plot(vDBinE, vDsampledBP[:,2], 'x', color = 'skyblue', label='image3')\n",
    "plt.plot(vDBg, pixelrange, color='blue')\n",
    "plt.legend()\n",
    "plt.title(\"Pixel value over log exposure (Blue) - gradient-drive sample schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Estimate a radiance map by selecting of blending pixels from different exposures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive way to measure exposures is\n",
    "\n",
    "$$\\log E_i   = g(z_{ij}) - \\log t_j$$\n",
    "\n",
    "Unfortunately, pixels are noisy, especially under low-light conditions when fewer photons arrive at the sensor. \n",
    "\n",
    "Mitsunaga and Nayar (1999) show that in order to maximize the signal-to-noise ratio (SNR), the weighting function must emphasize both higher pixel values and larger gradients in the transfer function\n",
    "$$\n",
    "w(z)=f^{-1}(z)/f^{'-1}(z)\n",
    "$$\n",
    "\n",
    "(Note that there is some confusion of the notation in the book, the measurement of weight function should based the inverse of reponse function **without logarithm**. Also, weight function should always be **positive**)\n",
    "\n",
    "\n",
    "the weights $w$ are used to form the final irradiance estimate\n",
    "\n",
    "$$\n",
    "\\log E_i = \\frac{\\sum_j w(z_{ij})[g(z_{ij})-\\log t_j]}{\\sum_j w(z_{ij})}\n",
    "$$\n",
    "\n",
    "We also multiply it by a hat distribution to further deter the use of clipped highlights and shadows from source images. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input:\n",
    "# g : the return data from g-solver \n",
    "# output:\n",
    "# result: the weight map\n",
    "def weightgenerate(g, clamp_extremes=True):\n",
    "    trans = np.exp(g)\n",
    "    gprime = np.gradient(trans)\n",
    "    result = np.array(trans/gprime)\n",
    "    weights = np.zeros(g.shape[0])\n",
    "    gmax = len(g)\n",
    "    midpoint = weights.size * 1/2\n",
    "    for i in range(weights.size):\n",
    "        if i < midpoint:\n",
    "            weights[i] = i\n",
    "        else:\n",
    "            weights[i] = weights.size - i\n",
    "    # Add a small value to ensure all weights are positive\n",
    "    # Otherwise, we may encounter division by zero in logEEstimate\n",
    "    if clamp_extremes:\n",
    "        return result * (weights + 1e-8)\n",
    "    else:\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input:\n",
    "# g : the return data from g-solver \n",
    "# weight: the return data from weightgenerate correspond to g\n",
    "# t: log shutter speed, for image j\n",
    "# imgPack: the return data from imagepack\n",
    "# output:\n",
    "# logE_i for each pixel - np array\n",
    "def logEEstimate(g,weight,t,imgPack):\n",
    "    size = imgPack.shape[0]\n",
    "    num_img = imgPack.shape[1]\n",
    "    result = np.zeros(size)\n",
    "    for i in range(size):\n",
    "        temp = 0\n",
    "        weightsum = 0\n",
    "        for j in range(num_img):\n",
    "            temp = temp + weight[int(imgPack[i,j])] * (g[int(imgPack[i,j])] - t[j])\n",
    "            weightsum = weightsum + weight[int(imgPack[i,j])]\n",
    "        result[i] = temp/weightsum\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib notebook\n",
    "## estimate g prime g' for each channel\n",
    "Rweight = weightgenerate(Rg)\n",
    "Gweight = weightgenerate(Gg)\n",
    "Bweight = weightgenerate(Bg)\n",
    "origRweight = weightgenerate(Rg, False)\n",
    "origGweight = weightgenerate(Gg, False)\n",
    "origBweight = weightgenerate(Bg, False)\n",
    "RglogE = logEEstimate(Rg,Rweight,[-2,0,2],RP)\n",
    "GglogE = logEEstimate(Gg,Gweight,[-2,0,2],GP)\n",
    "BglogE = logEEstimate(Bg,Bweight,[-2,0,2],BP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelrange = np.arange(256)\n",
    "plt.figure(figsize = (8, 12))\n",
    "plt.subplot(211)\n",
    "plt.plot(origRweight, pixelrange, color='red')\n",
    "plt.plot(origGweight, pixelrange, color='green')\n",
    "plt.plot(origBweight, pixelrange, color='blue')\n",
    "plt.title(\"Mitsunaga and Nayar Weight function\")\n",
    "plt.subplot(212)\n",
    "plt.plot(Rweight, pixelrange, color='red')\n",
    "plt.plot(Gweight, pixelrange, color='green')\n",
    "plt.plot(Bweight, pixelrange, color='blue')\n",
    "plt.title(\"Modified Weight function\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Tone map the resulting high dynamic range (HDR) image back into a displayable gamut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is usually necessary to display the HDR image on a lower gamut screen.\n",
    "\n",
    "1. Global Transfer Curve (i.e Gamma Curve) (Larson, Rushmeier, and Pattanaik 2005)\n",
    "    - If Gamma curve is applied seperate to each channel, then the color is less saturated \n",
    "    - If Gamma curve is applied to the luminance channel, then result is better. (the image is splited up into luminance and chrominance components $L^*a^*b^*$\n",
    "2. If the image has wide range of exposures, we can divide each pixel by the average brightness in a region around it, like dodging and burning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_img = np.zeros(im_auto_expose.shape)\n",
    "logEs = [RglogE, GglogE, BglogE]\n",
    "for i in range(3):\n",
    "    result_img[:,:,i] = (logEs[i].reshape(im_auto_expose.shape[0], im_auto_expose.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal2image(img, max_value = 255.):\n",
    "    ret = img.copy()\n",
    "    ret -= img.min()\n",
    "    ret *= max_value / ret.max()\n",
    "    return ret\n",
    "\n",
    "def image2normal(img):\n",
    "    min_value = img.min()\n",
    "    max_value = img.max()\n",
    "    normal = (img - img.min()) / (max_value - min_value)\n",
    "    return normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linearly maped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_linearMap = normal2image(result_img).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gamma(Vin, gamma = 0.5):\n",
    "    return np.power(Vin, 1 / gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gamma correction is, in the simplest cases, defined by the following power-law expression:\n",
    "\n",
    "$ V_{\\text{out}}=A{V_{\\text{in}}^{\\gamma }}$\n",
    "\n",
    "where the non-negative real input value $V_{\\text{in}}$ is raised to the power $\\gamma$  and multiplied by the constant A, to get the output value$V_{\\text{out}}$. In the common case of A = 1, inputs and outputs are typically in the range 0–1.\n",
    "\n",
    "#### Gamma applied to each color channel independentlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_normal = image2normal(result_img)\n",
    "img_gamColor = compute_gamma(img_normal)\n",
    "img_gamColor = normal2image(img_gamColor).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gamma applied to intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tonemap_gamma_intensity(img):\n",
    "    lab = color.rgb2lab(img)\n",
    "    l_channel = lab[:,:,0]\n",
    "    l_max = l_channel.max()\n",
    "    l_gamma = normal2image(compute_gamma(image2normal(l_channel)), l_max)\n",
    "    lab[:,:,0] = l_gamma\n",
    "    return img_as_ubyte(color.lab2rgb(lab))\n",
    "\n",
    "img_gamIntensity = tonemap_gamma_intensity(img_normal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dodging and burning (Linear Filters)\n",
    "def tonemap_dab(img):\n",
    "    lab2 = color.rgb2lab(img)\n",
    "    l_channel = lab2[:,:,0]\n",
    "    hh = np.log(l_channel)\n",
    "    h_low = skimage.filters.gaussian(hh,  sigma=100 , truncate=2.0)\n",
    "    h_high = hh - h_low\n",
    "    h = h_low + h_high * 0.1\n",
    "    l = np.exp(hh)\n",
    "    lab2[:,:,0] = l\n",
    "    return img_as_ubyte(color.lab2rgb(lab2))\n",
    "\n",
    "img_DaB = tonemap_dab(img_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSaturation(img):\n",
    "    return color.rgb2hsv(img)[:,:,1].mean()\n",
    "\n",
    "fig = plt.figure(figsize=(10,16))\n",
    "plt.subplot(221)\n",
    "plt.imshow(img_linearMap)\n",
    "plt.title('linear map, S = {:.4f}'.format(computeSaturation(img_linearMap)))\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.imshow(img_gamColor)\n",
    "plt.title('Gamma applied to each color channel, S = {:.4f}'.format(computeSaturation(img_gamColor)))\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.imshow(img_gamIntensity)\n",
    "plt.title('Gamma applied to intensity, S = {:.4f}'.format(computeSaturation(img_gamIntensity)))\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.imshow(img_DaB)\n",
    "plt.title('Dodge and Burn, S = {:.4f}'.format(computeSaturation(img_DaB)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance-Driven Sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radiances = [vDRg, vDGg, vDBg]\n",
    "samples = [RP, GP, BP]\n",
    "result_img2 = np.zeros(im_auto_expose.shape)\n",
    "\n",
    "for i in range(len(radiances)):\n",
    "    weight = weightgenerate(radiances[i])\n",
    "    result_img2[:,:,i] = (logEEstimate(radiances[i],weight,[-2,0,2],samples[i]).reshape(im_auto_expose.shape[0], im_auto_expose.shape[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_linearMap = normal2image(result_img2).astype(np.uint8)\n",
    "\n",
    "def tonemap_gamma_colour(img, gamma=None):\n",
    "    img_normal = image2normal(img)\n",
    "    if gamma:\n",
    "        img_gamColor = compute_gamma(img_normal, gamma)\n",
    "    else:\n",
    "        img_gamColor = compute_gamma(img_normal)\n",
    "    return normal2image(img_gamColor).astype(np.uint8)\n",
    "\n",
    "img_gamColor = tonemap_gamma_colour(result_img2)\n",
    "\n",
    "## lab\n",
    "def tonemap_gamma_luminosity(img, gamma=None):\n",
    "    lab = color.rgb2lab(img)\n",
    "    l_channel = lab[:,:,0]\n",
    "    l_max = l_channel.max()\n",
    "    if gamma:\n",
    "        img_gamColor = compute_gamma(image2normal(l_channel), gamma)\n",
    "    else:\n",
    "        img_gamColor = compute_gamma(image2normal(l_channel))\n",
    "    l_gamma = normal2image(img_gamColor, l_max)\n",
    "    lab[:,:,0] = l_gamma\n",
    "    return img_as_ubyte(color.lab2rgb(lab))\n",
    "\n",
    "img_gamIntensity = tonemap_gamma_luminosity(img_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,16))\n",
    "\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.imshow(im_auto_expose)\n",
    "plt.title('original image, S = {:.4f}'.format(computeSaturation(im_auto_expose)))\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.imshow(img_DaB)\n",
    "plt.title('linear map, S = {:.4f}'.format(computeSaturation(img_DaB)))\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.imshow(img_gamColor)\n",
    "plt.title('Gamma applied to each color channel, S = {:.4f}'.format(computeSaturation(img_gamColor)))\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.imshow(img_gamIntensity)\n",
    "plt.title('Gamma applied to intensity, S = {:.4f}'.format(computeSaturation(img_gamIntensity)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extentions\n",
    "-  multi-exposure images taken without a tripod using homography-based registration of the sequence\n",
    "- drop the assumption that exposure is known\n",
    "\n",
    "(both are metioned in the book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Drop the assumption that exposure is known\n",
    "\n",
    "Suppose $\\log t_j$ are unknowns in our least squares problem.\n",
    "\n",
    "$$E = \\sum_i\\sum_jw(z_{i,j})[g(z_{i,j}) - \\log E_i - \\log t_j]^2 + \\lambda\\sum_k g''(k)^2 + \\eta \\sum_j(t_j - \\hat{t}_j)^2$$\n",
    "\n",
    "In other word, we are solve the three equations together to estimate the radiometric response function $g$, irradiance values $E_i$ and $t_j$\n",
    "$$ w(z_{i,j}) g(z_{i,j}) - w(z_{i,j}) \\log E_i  - w(z_{i,j}) \\log t_j= 0 $$ \n",
    "$$\\lambda[g(z_{i,j}-1) - 2g(z_{i,j})+ g(z_{i,j}+1)] = 0$$\n",
    "$$ \\eta (t_j - \\hat{t}_j) = 0$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Modified from gsolve.m\n",
    "\n",
    "Solve for imaging system response function\n",
    "\n",
    "Given a set of pixel values observed for several pixels in several\n",
    "images with different exposure times, this function returns the\n",
    "imaging system’s response function\n",
    "\n",
    "\n",
    "Z(i,j): the pixel values of pixel location number i in image j\n",
    "\n",
    "l: lamdba, the constant that determines the amount of smoothness\n",
    "\n",
    "w(z): the weighting function value for pixel value z\n",
    "\n",
    "eta: the constraint for exposure time, should be positive for ascending exposure time sequence\n",
    "\n",
    "t_hat: the nominal value of default image (middle image in image sequence)\n",
    "'''\n",
    "\n",
    "def gsolve2(Z, lmd, eta = 10, t_hat = 0, w=weight_hat):\n",
    "\n",
    "    locations = Z.shape[0]\n",
    "    sequences = Z.shape[1]\n",
    "    n = 256  # [0, 255]\n",
    "    A = np.zeros((locations * sequences + n + sequences - 1, n + locations + sequences), dtype=float)\n",
    "    b = np.zeros(A.shape[0], dtype=float)\n",
    "\n",
    "    #  Include the data−fitting equations\n",
    "    k = 0\n",
    "    for i in range(locations):\n",
    "        for j in range(sequences): \n",
    "            wij = w(Z[i, j])\n",
    "            A[k, int(Z[i, j])] = wij\n",
    "            A[k, n + i] = -wij\n",
    "            A[k, n + locations + j] = -wij #b[k] = wij * B[j]\n",
    "            k += 1\n",
    "\n",
    "    # Fix the curve by setting its middle value to 0, i.e. g(128) = 0    \n",
    "    A[k, 128] = 1   \n",
    "    k += 1\n",
    "    \n",
    "    # Include the smoothness equations\n",
    "    for i in range(n-2): \n",
    "        wi = w(i + 1)\n",
    "        A[k, i] = lmd * wi\n",
    "        A[k, i+1] = -2 * lmd * wi\n",
    "        A[k, i+2] = lmd * wi\n",
    "        k += 1    \n",
    "    \n",
    "    # exposure time constraints\n",
    "    for i in range(sequences):\n",
    "        A[k, n + locations + i] = eta \n",
    "        b[locations * sequences + n + i - 1] =  eta * (i - sequences//2 + t_hat)\n",
    "        k += 1\n",
    " \n",
    "    # Solve the system\n",
    "    x = np.linalg.lstsq(A, b)[0]\n",
    "    g = x[:n]\n",
    "    lnE = x[n:n + locations]\n",
    "    lnt = x[n + locations:]\n",
    "\n",
    "    return (g, lnE, lnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silimilarly, for $N$ sample pixels in each image and $P$ images, we need $N \\times P > (Z_{max} -Z_{min}) + N + P$\n",
    "Suppose we have 3 images, $2N > 255 + 2$, $N > 129 $ should be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmd = 100\n",
    "\n",
    "targetsize = 1000\n",
    "eta = 100\n",
    "sampledRP = windowSample(RP,targetsize)\n",
    "sampledGP = windowSample(GP,targetsize)\n",
    "sampledBP = windowSample(BP,targetsize)\n",
    "R_g, R_linE, R_lnt = gsolve2(sampledRP, lmd, eta = eta)\n",
    "G_g, G_linE, G_lnt = gsolve2(sampledGP, lmd, eta = eta)\n",
    "B_g, B_linE, B_lnt = gsolve2(sampledBP, lmd, eta = eta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.plot(R_g, np.arange(256), color = \"r\")\n",
    "plt.plot(G_g, np.arange(256), color = \"g\")\n",
    "plt.plot(B_g, np.arange(256), color = \"b\")\n",
    "plt.title(\"Pixel value over log exposure - unknown exposure time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will demo our apporoach also works with a longer sequence whose EV is not arithmetic sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cave_prefix = 'images/cave/agia-sofia_'\n",
    "\n",
    "cave_images = np.array([image.imread(cave_prefix + \"-4.0.jpg\"),\n",
    "                       image.imread(cave_prefix + \"-2.7.jpg\"),\n",
    "                       image.imread(cave_prefix + \"-2.jpg\"),\n",
    "                       image.imread(cave_prefix + \"-1.4.jpg\"),\n",
    "                       image.imread(cave_prefix + \"-0.7.jpg\"),\n",
    "                       image.imread(cave_prefix + \"0.jpg\"),\n",
    "                       image.imread(cave_prefix + \"+1.jpg\"),\n",
    "                       image.imread(cave_prefix + \"+2.jpg\"),\n",
    "                       image.imread(cave_prefix + \"+3.jpg\"),\n",
    "                       image.imread(cave_prefix + \"+4.jpg\"),\n",
    "                       image.imread(cave_prefix + \"+5.jpg\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cave_RP,cave_GP,cave_BP = imagepack(cave_images) \n",
    "\n",
    "lmd = 100\n",
    "eta = 100\n",
    "\n",
    "cave_sampledRP = windowSample(cave_RP, 100)\n",
    "cave_sampledGP = windowSample(cave_GP, 100)\n",
    "cave_sampledBP = windowSample(cave_BP, 100)\n",
    "cave_R_g, cave_R_linE, cave_R_lnt = gsolve2(cave_sampledRP, lmd, eta = eta)\n",
    "cave_G_g, cave_G_linE, cave_G_lnt = gsolve2(cave_sampledGP, lmd, eta = eta)\n",
    "cave_B_g, cave_B_linE, cave_B_lnt = gsolve2(cave_sampledBP, lmd, eta = eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "\n",
    "for i in range(cave_images.shape[0]):\n",
    "    plt.plot(cave_R_linE + cave_R_lnt[i], cave_sampledRP[:,i], 'x')\n",
    "    \n",
    "plt.plot(cave_R_g, np.arange(256), color = 'black')\n",
    "plt.title('Estimated pixel value over log exposure (Red)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cave_lnt = np.sum([cave_R_lnt, cave_G_lnt, cave_B_lnt], axis = 0) / 3.0\n",
    "real_cave_lnt = np.array([-4.0, -2.7, -2.0, -1.4, -0.7, 0, 1, 2, 3,4 ,5])\n",
    "np.set_printoptions(precision=1)\n",
    "print(\"The real lnt sequence are\")\n",
    "print(real_cave_lnt)\n",
    "print(\"The estimated lnt are\")\n",
    "print(cave_lnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cave_R_weight = weightgenerate(cave_R_g)\n",
    "cave_G_weight = weightgenerate(cave_G_g)\n",
    "cave_B_weight = weightgenerate(cave_B_g)\n",
    "cave_RglogE = logEEstimate(cave_R_g,cave_R_weight,cave_lnt,cave_RP)\n",
    "cave_GglogE = logEEstimate(cave_G_g,cave_G_weight,cave_lnt,cave_GP)\n",
    "cave_BglogE = logEEstimate(cave_B_g,cave_B_weight,cave_lnt,cave_BP)\n",
    "\n",
    "cave_result_img = np.zeros(cave_images[0].shape)\n",
    "cave_logEs = [cave_RglogE, cave_GglogE, cave_BglogE]\n",
    "for i in range(3):\n",
    "    cave_result_img[:,:,i] = (cave_logEs[i].reshape(cave_result_img.shape[0], cave_result_img.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cave_img_gamColor = tonemap_gamma_colour(cave_result_img, 0.7)\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.imshow(cave_img_gamColor)\n",
    "plt.title(\"Image recovered with unknown exposure time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Images taken without tripod\n",
    "When multiple images are taken at varying orientations and exposures, we need to first align the input. The global alignment method should be tolerant to exposure differences. \n",
    "\n",
    "We use Horris corner for feature matches, which is invariance to intensity shift. Then,  we use RANSAC to make the  matching images are geometrically consistent. The function is implemented as `produceMatches`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.feature import produceMatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sample images are taken from https://www.ptgui.com/hdrtutorial.html\n",
    "(C) Copyright 2006 by Joost Nieuwenhuijse\n",
    "''' \n",
    "\n",
    "pano_prefix = 'images/panoramas/IMG_0'\n",
    "a = []\n",
    "for i in range(475, 487):\n",
    "    a.append(image.imread(\"images/panoramas/IMG_0{}.JPG\".format(i)))\n",
    "    \n",
    "pano_images = np.array(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can perform a global alignment and crop invalid margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropImage(imgs, margin = 25):\n",
    "    min_r = 0\n",
    "    min_c = 0\n",
    "    max_r = imgs[0].shape[0]\n",
    "    max_c = imgs[0].shape[1]\n",
    "    \n",
    "    for img in imgs:\n",
    "        intensity = np.sum(img, axis = 2) \n",
    "        image_co = np.argwhere(intensity > 0)\n",
    "        min_cor = np.min(image_co, axis = 0)\n",
    "        max_cor = np.max(image_co, axis = 0)\n",
    "        min_r = max(min_r, min_cor[0])\n",
    "        min_c = max(min_c, min_cor[1])\n",
    "        max_r = min(max_r, max_cor[0])\n",
    "        max_c = min(max_c, max_cor[1])\n",
    "               \n",
    "    crop_imgs = []\n",
    "    for img in imgs:\n",
    "        im = (np.uint8((img[min_r + margin : max_r - margin, min_c + margin: max_c -margin, :])*255))\n",
    "        crop_imgs.append(im)\n",
    "        \n",
    "    return np.array(crop_imgs)\n",
    "\n",
    "\n",
    "def produceAlignedImages(imgs, output_shape = None, panoramas = False, overlap_size = None):\n",
    "    ref_img = imgs[0]\n",
    "    warp_imgs = []\n",
    "    #warp_imgs.append(ref_img.astype(np.float64))\n",
    "    for i in range(imgs.shape[0]):\n",
    "        matchesLR, model_robust, inliers = produceMatches(ref_img, imgs[i], panoramas, overlap_size)\n",
    "        if (output_shape == None):\n",
    "            warp_img = warp(imgs[i], model_robust)\n",
    "        else:\n",
    "            warp_img = warp(imgs[i], model_robust, output_shape = output_shape)\n",
    "        warp_imgs.append(warp_img)\n",
    "        \n",
    "    warp_imgs = np.array(warp_imgs)\n",
    "        \n",
    "    return cropImage(warp_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_images = produceAlignedImages(pano_images[6:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can compare radiance reponse between aligned image input and original image input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pano_lnt = [0, -2, 2]\n",
    "\n",
    "lmd = 100\n",
    "targetsize = 1000 \n",
    "\n",
    "pano_lnEs = []\n",
    "pano_gs = []\n",
    "pano_packs = imagepack(crop_images)\n",
    "for pack in pano_packs:\n",
    "    sample = windowSample(pack, targetsize)\n",
    "    g, lnE = gsolve(sample, pano_lnt, lmd)\n",
    "    pano_lnEs.append(lnE)\n",
    "    pano_gs.append(g)\n",
    "    \n",
    "pano_packs_ori = imagepack(pano_images[6:9])\n",
    "pano_lnEs_ori = []\n",
    "pano_gs_ori = []\n",
    "for pack in pano_packs_ori:\n",
    "    sample = windowSample(pack, targetsize)\n",
    "    g, lnE = gsolve(sample, pano_lnt, lmd)\n",
    "    pano_lnEs_ori.append(lnE)\n",
    "    pano_gs_ori.append(g)\n",
    "\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(pano_gs[0], np.arange(256), color='r')\n",
    "plt.plot(pano_gs[1], np.arange(256), color='g')\n",
    "plt.plot(pano_gs[2], np.arange(256), color='b')\n",
    "plt.xlabel('Estimated log Exposure')\n",
    "plt.ylabel('Pixel Value')\n",
    "plt.title('Aligned input')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(pano_gs_ori[0], np.arange(256), color='r')\n",
    "plt.plot(pano_gs_ori[1], np.arange(256), color='g')\n",
    "plt.plot(pano_gs_ori[2], np.arange(256), color='b')\n",
    "plt.xlabel('Estimated log Exposure')\n",
    "plt.ylabel('Pixel Value')\n",
    "plt.title('Original input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResultImage(gs, packs, lnt, img_shape):\n",
    "    glogEs = []\n",
    "    for i in range(3):\n",
    "        weight = weightgenerate(gs[i])\n",
    "        glogE = logEEstimate(gs[i], weight, lnt, packs[i])\n",
    "        glogEs.append(glogE)\n",
    "        \n",
    "    result_img = np.zeros(img_shape)\n",
    "    for i in range(3):\n",
    "        result_img[:,:,i] = (glogEs[i].reshape(img_shape[0], img_shape[1]))\n",
    "        \n",
    "    return result_img  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pano_result_img = getResultImage(pano_gs, pano_packs, [0,-2,2], crop_images[0].shape)\n",
    "pano_unaligned_result_img = getResultImage(pano_gs_ori, pano_packs_ori, [0,-2,2], pano_images[6].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there is blurriness in the unaligned images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(tonemap_gamma_colour(pano_result_img, 0.7))\n",
    "plt.title('Aligned result')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(tonemap_gamma_colour(pano_unaligned_result_img, 0.7))\n",
    "plt.title('Original result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce HDR Panoramas\n",
    "To reproduce HDR Panoramas, we can compute each pixel by the radiance value response function from the overlapping images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_images2 = produceAlignedImages(pano_images[9:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = crop_images[0].shape[1] * 2 \n",
    "height = crop_images[0].shape[0] \n",
    "\n",
    "_, model_pano, _ = produceMatches(crop_images[0], crop_images2[0], panoramas = True, overlap_size = 400)\n",
    "\n",
    "tform = SimilarityTransform(scale = 1.0)\n",
    "\n",
    "crop_images12 = []\n",
    "for img in crop_images:\n",
    "    crop_images12.append(warp(img, tform, output_shape=(height, width)))\n",
    "\n",
    "for img in crop_images2:\n",
    "    crop_images12.append(warp(img, model_pano, output_shape = (height, width)))\n",
    "\n",
    "crop_images12 = np.array(crop_images12)\n",
    "\n",
    "resultsss = cropImage(crop_images12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pano_lnt2 = [0, -2, 2, 0, -2, 2]\n",
    "\n",
    "lmd = 100\n",
    "targetsize = 1000 \n",
    "\n",
    "pano_lnEs2 = []\n",
    "pano_gs2 = []\n",
    "pano_packs2 = imagepack(resultsss)\n",
    "for pack in pano_packs2:\n",
    "    sample = windowSample(pack, targetsize)\n",
    "    g, lnE = gsolve(sample, pano_lnt2, lmd)\n",
    "    pano_lnEs2.append(lnE)\n",
    "    pano_gs2.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pano_result_img1 = getResultImage(pano_gs2, imagepack(crop_images), [0,-2,2], crop_images[0].shape)\n",
    "pano_result_img2 = getResultImage(pano_gs2, imagepack(crop_images2), [0,-2,2], crop_images2[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.blend import boundaryDT, alpha\n",
    "\n",
    "gam_result_img = tonemap_gamma_colour(pano_result_img1, 0.8)\n",
    "gam_result_img2 = tonemap_gamma_colour(pano_result_img2, 0.8)\n",
    "\n",
    "imWarpL = warp(gam_result_img, tform, output_shape=(height, width))\n",
    "imWarpR = warp(gam_result_img2, model_pano, output_shape = (height, width))\n",
    "\n",
    "boundaryDTL = warp(boundaryDT(gam_result_img), tform, output_shape=(height, width))\n",
    "boundaryDTR = warp(boundaryDT(gam_result_img2), model_pano, output_shape=(height, width))\n",
    "alphaValue = alpha(boundaryDTL, boundaryDTR, height, width)\n",
    "\n",
    "imAlphaL = np.zeros(imWarpL.shape)\n",
    "imAlphaR = np.zeros(imWarpR.shape)\n",
    "for i in range (3):\n",
    "    imAlphaL[:,:,i] = alphaValue[0]\n",
    "    imAlphaR[:,:,i] = alphaValue[1]\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "plt.imshow(np.multiply(imWarpL ,imAlphaL)[100:,:-400,:] + np.multiply(imWarpR, imAlphaR)[100:,:-400,:])\n",
    "plt.title(\"Panorama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions \n",
    "\n",
    "We have implemented Debevec-Malik method to recover high dynamic range radiance maps from ordinary photographs. When we use different sample methods to reconstruct the response function, we obtain slightly different results. For the window sampling schema, it more or less preserves some spatial information as we sample among each kernel of fixed size. Also, the probably of each pixel gets sampled only depends on its location. There is not preference for pixels of certain intensity. In this case, our reconstructed response function is a continuous curve as pixels of almost all intensities get sampled and contribute to reconstruction. However, from the final result, the reconstructed curve is noisy. This is due to pixels from region of high-variance will have the effect of optical blur and will disturb our reconstruction. In this case, we use a biased sampling method, namely variance-driven sampling (detail see above). Since we have lower probability to sample points of high-variance, our reconstructed curve is less noisy.However, in this sampling method, we are at risk of missing pixels of certain intensities. In this case, our reconstruction curve will be not representative to some extent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "Debevec, P. E., & Malik, J. (1997). Recovering high dynamic range radiance maps from photographs. Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques - SIGGRAPH 97. doi: 10.1145/258734.258884\n",
    "\n",
    "Eden, A., Uyttendaele, M., & Szeliski, R. (2006). Seamless Image Stitching of Scenes with Large Motions and Exposure Differences. 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2 (CVPR06). doi: 10.1109/cvpr.2006.268\n",
    "\n",
    "Szeliski, R. (2011). Computer Vision Algorithms and Applications. London: Springer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
